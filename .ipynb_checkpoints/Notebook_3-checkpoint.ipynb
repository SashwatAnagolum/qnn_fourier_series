{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A topological analysis (?) of quantum neural networks\n",
    "\n",
    "## Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "\n",
    "import cirq\n",
    "import sympy\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors, DistanceMetric\n",
    "from sklearn.utils import graph_shortest_path\n",
    "\n",
    "# from ripser import ripser\n",
    "\n",
    "# import tadasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cirq.GridQubit(0, 0), cirq.GridQubit(0, 1), cirq.GridQubit(0, 2), cirq.GridQubit(0, 3)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[ 0.7701476 , -0.57848626],\n",
       "       [ 0.7701476 , -0.57848626],\n",
       "       [ 0.7701476 , -0.57848626],\n",
       "       [ 0.7701476 , -0.57848626]], dtype=float32)>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_qubits = 4\n",
    "layers = [ry_layer, c_layer, ry_layer]\n",
    "extra_params = [[None], ['cx', 'linear', num_qubits], [None]]\n",
    "weight_bounds = [0, 4, 4, 8]\n",
    "weights = sympy.symbols(' '.join(['theta_' + str(i) for i in range(weight_bounds[-1])]))\n",
    "\n",
    "qnn = tfq_qnn_generator(num_qubits, layers, extra_params, weight_bounds, weights)\n",
    "q_layer = tfq.layers.PQC(qnn, [cirq.Z(q[i]) for i in range(2)])\n",
    "q_layer(tfq.convert_to_tensor([cirq.Circuit(), cirq.Circuit(), cirq.Circuit(), cirq.Circuit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(np.random.sample(2)) for i in range(5000)]\n",
    "data = [[basic_embedding(i), 1] for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.array(data)\n",
    "# labels = data[:, -1]\n",
    "# feats = data[: , 0]\n",
    "# tensors = tfq.convert_to_tensor(feats)\n",
    "tensors = np.array(tensors).T\n",
    "\n",
    "# import time\n",
    "# start = time.time()\n",
    "# q_layer(tensors)\n",
    "# # print(time.time() - start)\n",
    "# print(q_layer(tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((tensors, labels)).reshape(2, 5000).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = CustomDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Circuit layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ry_layer(circ, q, _, params):\n",
    "    for i, theta in enumerate(params):\n",
    "        circ.append(cirq.ry(theta)(q[i]))\n",
    "    \n",
    "\n",
    "def rx_layer(circ, q, _, params):\n",
    "     for i, theta in enumerate(params):\n",
    "        circ.append(cirq.rx(theta)(q[i]))\n",
    "        \n",
    "\n",
    "def rz_layer(circ, q, _, params):\n",
    "     for i, theta in enumerate(params):\n",
    "        circ.append(cirq.rz(theta)(q[i]))\n",
    "        \n",
    "\n",
    "def cr_layer(circ, q, gate, cr_map, num_qubits, params):\n",
    "    gate = {'crz': qml.CRZ, 'cry': qml.CRY, 'crx': qml.CRX}[gate]\n",
    "\n",
    "    if cr_map == 'linear':\n",
    "        for i, theta in enumerate(params):\n",
    "            gate(theta, wires=[i, i + 1])\n",
    "    elif cr_map == 'full':\n",
    "        for i in range(num_qubits - 1):\n",
    "            offset = sum(range(num_qubits - i, num_qubits))\n",
    "            for j in range(num_qubits - i - 1):\n",
    "                gate(params[offset + j], wires=[i, i + j + 1])\n",
    "\n",
    "\n",
    "def hadamard_layer(circ, q, num_qubits, _):\n",
    "    for i in range(num_qubits):\n",
    "        circ.append(cirq.H(q[i]))\n",
    "        \n",
    "\n",
    "def c_layer(circ, q, gate, c_map, num_qubits, _):\n",
    "    gate = {'cz': cirq.CZ, 'cx': cirq.CNOT}[gate]\n",
    "    \n",
    "    if c_map == 'full':\n",
    "        for i in range(num_qubits - 1):\n",
    "            for j in range(num_qubits - i - 1):\n",
    "                circ.append(gate(q[i], q[i + j + 1]))   \n",
    "    else:\n",
    "        for i in range(num_qubits - 1):\n",
    "            circ.append(gate(q[i], q[i + 1]))\n",
    "\n",
    "        if c_map == 'circular':\n",
    "            circ.append(gate(q[num_qubits - 1], q[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfq_qnn_generator(num_qubits, layers, layer_extra_params, weights_boundaries, weights):\n",
    "    circ = cirq.Circuit()\n",
    "    q = cirq.GridQubit.rect(1, num_qubits) \n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        layer(circ, q, *layer_extra_params[i], weights[weights_boundaries[i]: weights_boundaries[i + 1]])     \n",
    "    \n",
    "    return circ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_embedding(data):\n",
    "    circ = cirq.Circuit()\n",
    "    q = cirq.GridQubit.rect(1, 4)\n",
    "    hadamard_layer(circ, q, 4, [])\n",
    "    ry_layer(circ, q, [], data)\n",
    "    c_layer(circ, q, 'cx', 'linear', 4, [])\n",
    "    circ.append(cirq.ry(data[0] * data[1])(q[0]))\n",
    "    rx_layer(circ, [q[1], q[3]], [], data)\n",
    "    c_layer(circ, q, 'cx', 'linear', 4, [])    \n",
    "    rx_layer(circ, [q[0], q[2]], [], data)\n",
    "    \n",
    "    return circ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch NNs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalNN(nn.Module):\n",
    "    def __init__(self, layers_dimensions, use_softmax=True):\n",
    "        super(ClassicalNN, self).__init__()\n",
    "        layers_list = [nn.Linear(layers_dimensions[0][0], layers_dimensions[0][1])]\n",
    "        \n",
    "        for layer_dims in layers_dimensions[1:-1]:\n",
    "            layers_list.append(nn.Linear(layer_dims[0], layer_dims[1]))\n",
    "            layers_list.append(nn.ReLU())\n",
    "            \n",
    "        if len(layers_dimensions) > 1:\n",
    "            layers_list.append(nn.Linear(layers_dimensions[-1][0], layers_dimensions[-1][1]))\n",
    "            \n",
    "        if use_softmax:\n",
    "            layers_list.append(nn.Softmax(dim=1))\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            *layers_list\n",
    "        )\n",
    "        \n",
    "        self.loss_fn = nn.BCELoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        preds = self.layers(x)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        return self.loss_fn(x, y)\n",
    "    \n",
    "    \n",
    "class TorchQNN(nn.Module):\n",
    "    def __init__(self, qlayer):\n",
    "        super(TorchQNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            qlayer\n",
    "        )\n",
    "        \n",
    "        self.loss_fn = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        preds = self.layers(x)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        return self.loss_fn(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightClipper(object):\n",
    "    def __init__(self, frequency=5):\n",
    "        self.frequency = frequency\n",
    "        \n",
    "    def __call__(self, module):\n",
    "        print(list(module.parameters()))\n",
    "        for param in module.parameters():\n",
    "            param = torch.remainder(param, 2 * np.pi)\n",
    "\n",
    "\n",
    "def train_nn(model, data_loader, test_data_loader, optimizer, num_epochs, verbose=False):\n",
    "    size = len(data_loader.dataset)\n",
    "    clipper = WeightClipper()\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        for batch, (x, y) in enumerate(data_loader):\n",
    "            preds = model(x.float())\n",
    "            loss = model.loss(preds.float(), y.float())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose:\n",
    "                if batch % 2 == 0:\n",
    "                    loss = loss.item()\n",
    "                    print('Loss: {:>5f} | Samples: {:>5d} / {:>5d}'.format(loss, batch * len(x), size))\n",
    "                    \n",
    "                    \n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            acc = 0\n",
    "            test_size = len(test_data_loader.dataset)\n",
    "\n",
    "            for _, (x, y) in enumerate(test_data_loader):\n",
    "                pred = model(x.float())\n",
    "                test_loss += model.loss(pred, y.float())\n",
    "                acc +=  1 if y.argmax() == pred.argmax() else 0\n",
    "                if (i + 1) == num_epochs:\n",
    "                    print('Sample: {} | Pred: {}'.format(x, pred))\n",
    "\n",
    "            acc /= test_size\n",
    "            test_loss /= test_size\n",
    "            print('Iteration: {} | Training Loss: {} | Test Loss: {} | Accuracy: {} '.format(i + 1, loss, test_loss, acc))\n",
    "            \n",
    "            if acc >= 0.999:\n",
    "                return acc\n",
    "            \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data_loader, circ, verbose=False, weights=None):\n",
    "    if isinstance(weights, np.ndarray):\n",
    "        fn = lambda x: circ(x[0], weights)\n",
    "    else:\n",
    "        fn = circ\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(data_loader):\n",
    "            pred = fn(x.float())\n",
    "            if verbose:\n",
    "                print('Label: {} | Pred: {}'.format(y.numpy(), pred.numpy()))\n",
    "            preds.append(pred.numpy()[0])\n",
    "       \n",
    "    return np.array(preds)\n",
    "\n",
    "\n",
    "def compute_performance_metrics(preds, labels, return_type):\n",
    "    if return_type == 0:\n",
    "        thresholded_preds = 2 * (preds > 0) - 1\n",
    "        classifications = 0.5 * ((thresholded_preds * labels) + 1)\n",
    "        acc = np.sum(classifications) / classifications.shape[0]\n",
    "        tp = 2 * np.sum(classifications[:classifications.shape[0] // 2]) / classifications.shape[0]\n",
    "        tn = 2 * np.sum(classifications[classifications.shape[0] // 2:]) / classifications.shape[0]\n",
    "    elif return_type == 2:\n",
    "        print(preds)\n",
    "    \n",
    "    return acc, tp, tn\n",
    "    \n",
    "\n",
    "def plot_2d_prediction_landscape(data, circ, resolution=30):\n",
    "    features = np.linspace(-1, 1, resolution)\n",
    "    points = []\n",
    "    preds = []\n",
    "\n",
    "    for i in features:\n",
    "        for j in features:\n",
    "            points.append(np.array([i, j, 0]))\n",
    "\n",
    "    points = CustomDataset(np.array(points))\n",
    "    points_loader = DataLoader(points, batch_size=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(points_loader):\n",
    "            preds.append(circ(x.float()).numpy()[0])\n",
    "                      \n",
    "    preds = np.array(preds)[:, 0].reshape(resolution, resolution)\n",
    "    vals = ((-1 * preds) + 1) / 2\n",
    "\n",
    "    plt.figure(figsize=[16, 9])\n",
    "    fig, ax1 = plt.subplots()\n",
    "    im = ax1.imshow(vals, cmap='RdBu', extent=[-1.05, 1.05, -1.05, 1.05])\n",
    "    cbar = ax1.figure.colorbar(im, ax=ax1)\n",
    "#     im = ax2.imshow(vals, cmap='RdBu', extent=[-1.05, 1.05, -1.05, 1.05])\n",
    "#     ax2.plot(data[:data.shape[0] // 2, 0], data[:data.shape[0] // 2, 1], 'ro')\n",
    "#     ax2.plot(data[data.shape[0] // 2:, 0], data[data.shape[0] // 2:, 1], 'bo')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_circles(angle1, angle2, angle3, noise, separation, num_samples):\n",
    "    class_0_angles = np.concatenate((np.random.uniform(separation, angle1 - separation, num_samples // 2), \n",
    "                                     np.random.uniform(angle2 + separation, angle3 - separation, (num_samples + 1) // 2)))\n",
    "    class_1_angles = np.concatenate((np.random.uniform(angle1 + separation, angle2 - separation, num_samples // 2), \n",
    "                                     np.random.uniform(angle3 + separation, 2 * np.pi - separation, (num_samples + 1) // 2)))\n",
    "    \n",
    "    class_0_radii = np.random.uniform(-noise, noise, num_samples) + 1\n",
    "    class_1_radii = np.random.uniform(-noise, noise, num_samples) + 1\n",
    "\n",
    "    class_0 = np.concatenate((\n",
    "        class_0_radii * np.cos(class_0_angles), class_0_radii * np.sin(class_0_angles), \n",
    "        np.ones(num_samples)))\n",
    "\n",
    "    class_1 = np.concatenate((\n",
    "        class_1_radii * np.cos(class_1_angles), class_1_radii * np.sin(class_1_angles), \n",
    "        -1 * np.ones(num_samples)))\n",
    "\n",
    "    class_0 = np.reshape(class_0, (3, num_samples)).T    \n",
    "    class_1 = np.reshape(class_1, (3, num_samples)).T\n",
    "    data = np.concatenate((class_0, class_1))\n",
    "\n",
    "    x_max = np.max(data[:, 0])\n",
    "    x_min = np.min(data[:, 0])\n",
    "    y_max = np.max(data[:, 1])\n",
    "    y_min = np.min(data[:, 1])\n",
    "    data[:, 0] = (data[:, 0] - x_min) / (x_max - x_min)\n",
    "    data[:, 1] = (data[:, 1] - y_min) / (y_max - y_min)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def iris_subset():\n",
    "    data = np.genfromtxt('./processedIRISData.csv', delimiter=',', requires_grad=False)\n",
    "    data[:len(data) // 2, -1] = 1\n",
    "    data[len(data) // 2:, -1] = -1\n",
    "    \n",
    "    for i in range(4):\n",
    "        data[:, i] = (data[:, i] - np.min(data[:, i])) / (np.max(data[:, i]) - np.min(data[:, i]))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def embedded_discs(num_samples, num_centers):\n",
    "    samples_per_circle = int(num_samples * 9 / ((num_centers + 1) * 24)) + 1\n",
    "    radii = np.linspace(0, 1, 100)\n",
    "    centers = [[0.7 * np.cos(i), 0.7 * np.sin(i), 0] for i in np.linspace(\n",
    "        0, 2 * np.pi, num_centers, endpoint=False)] + [[0, 0, 0]]\n",
    "\n",
    "    separation_radii = 0.2\n",
    "    class_1_radii = 0.1\n",
    "    \n",
    "    data = []\n",
    "    data += [tadasets.dsphere(int(samples_per_circle * 2 * np.pi * radius), 1, radius) for i, radius in enumerate(radii)]\n",
    "    data = np.concatenate((data))\n",
    "    data = np.concatenate((data.T, np.ones((1, data.shape[0])))).T\n",
    "    \n",
    "    corrected_data = np.array([np.copy(data) - np.array(center) for center in centers])\n",
    "    center_dists = np.array([np.sqrt(np.sum(view[:, :-1] ** 2, axis=1))for view in corrected_data])\n",
    "    class_0_filters = np.array([center_dist > separation_radii for center_dist in center_dists])\n",
    "    class_0_filter = np.sum(class_0_filters, 0) == class_0_filters.shape[0]\n",
    "    class_1_filters = np.array([center_dist > class_1_radii for center_dist in center_dists])\n",
    "    class_1_filter = np.sum(class_1_filters, 0) < class_1_filters.shape[0]\n",
    "    \n",
    "    class_0 = np.random.permutation(data[class_0_filter])[:num_samples]\n",
    "    data[class_1_filter, -1] = -1\n",
    "    class_1 = np.random.permutation(data[class_1_filter])[:num_samples]\n",
    "\n",
    "    test_data = np.concatenate((\n",
    "        class_0[int(0.75 * num_samples):], class_1[int(0.75 * num_samples):]))\n",
    "    \n",
    "    train_data = np.concatenate((\n",
    "        class_0[:int(0.75 * num_samples)], class_1[:int(0.75 * num_samples)]))\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def concentric_spheres(num_samples, num_spheres):\n",
    "    centers = []\n",
    "    if num_spheres % 2:\n",
    "        centers += [[0, 0, 0, 0]]\n",
    "        \n",
    "    radius = 0.3\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, num_spheres // 2, endpoint=False)\n",
    "    centers += [[np.cos(angle), np.sin(angle), -1, 0] for angle in angles]\n",
    "    centers += [[np.cos(angle), np.sin(angle), 1, 0] for angle in angles]\n",
    "    centers = np.array(centers) * (1 - radius)\n",
    "\n",
    "    points_per_sphere = (num_samples // num_spheres) + 1\n",
    "    \n",
    "    class_0_template = np.concatenate((np.concatenate((tadasets.sphere(points_per_sphere // 2, radius / 3).T, \n",
    "                                                   np.ones((1, points_per_sphere // 2)))).T, \n",
    "                              np.concatenate((tadasets.sphere((points_per_sphere + 1) // 2, radius).T, \n",
    "                                              np.ones((1, (points_per_sphere + 1) // 2)))).T))\n",
    "    \n",
    "    class_1_template = np.concatenate((tadasets.sphere(points_per_sphere, radius * 2 / 3).T, \n",
    "                                   -1 * np.ones((1, points_per_sphere)))).T\n",
    "    \n",
    "    reduction = (points_per_sphere * 9) - num_samples\n",
    "    sphere_points = [points_per_sphere - 1 for i in range(reduction)] + [points_per_sphere \n",
    "                                                                          for i in range(reduction, num_spheres)]\n",
    "    \n",
    "    class_0_data = np.random.permutation(np.concatenate([class_0_template[:sphere_points[i]] + center \n",
    "                                   for i, center in enumerate(centers)]))\n",
    "    \n",
    "    class_1_data = np.random.permutation(np.concatenate([class_1_template[:sphere_points[i]] + center \n",
    "                                   for i, center in enumerate(centers)]))\n",
    "\n",
    "    test_data = np.concatenate((\n",
    "        class_0_data[int(0.375 * num_samples):int(0.5 * num_samples)], \n",
    "        class_0_data[int(0.875 * num_samples):], class_1_data[int(0.75 * num_samples):]))\n",
    "    \n",
    "    train_data = np.concatenate((\n",
    "        class_0_data[:int(0.375 * num_samples)], \n",
    "        class_0_data[int(0.5 * num_samples):int(0.875 * num_samples)], class_1_data[:int(0.75 * num_samples)]))\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset generation testing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = concentric_spheres(50000, 9)\n",
    "\n",
    "fig = plt.figure(figsize=[30, 30])\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax.scatter(train[:len(train) // 2, 0], train[:len(train) // 2, 1], train[:len(train) // 2, 2], c='coral')\n",
    "ax.scatter(train[(len(train) // 2):, 0], train[(len(train) // 2):, 1], train[(len(train) // 2):, 2], c='midnightblue')\n",
    "ax.view_init(45, 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = embedded_discs(50000, 9)\n",
    "\n",
    "fig = plt.figure(figsize=[30, 30])\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(train[:len(train) // 2, 0], train[:len(train) // 2, 1],c='coral')\n",
    "ax.scatter(train[(len(train) // 2):, 0], train[(len(train) // 2):, 1], c='midnightblue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, is_qnn=True):\n",
    "        self.data = data\n",
    "        self.feats = self.data[:, :-1]\n",
    "        self.temp_labels = self.data[:, -1]\n",
    "        if is_qnn:\n",
    "            self.labels = np.concatenate((-1 * self.temp_labels, self.temp_labels)).reshape(2, len(self.data)).T\n",
    "        else:\n",
    "            self.labels = np.concatenate(((1 + self.temp_labels) / 2, \n",
    "                                          (1 - self.temp_labels) / 2)).reshape(2, len(self.data)).T\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        samples = self.feats[idx]\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "        return samples, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance metric and distance matrix computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_knn_graph(dms, k):\n",
    "    knn = NearestNeighbors(n_neighbors=k, algorithm='auto', metric='precomputed')\n",
    "    knn_graphs = []\n",
    "    for dm in dms:\n",
    "        knn.fit(dm)\n",
    "        knn_graphs.append(knn.kneighbors_graph(dm, k).toarray())\n",
    "    \n",
    "    return np.array(knn_graphs)\n",
    "\n",
    "\n",
    "def trace_dist(state1, state2):\n",
    "    return 0.5 * np.trace(np.abs(np.outer(np.conj(state1.T), state1) - np.outer(np.conj(state2.T), state2))).real\n",
    "\n",
    "\n",
    "def hilbert_schmidt_metric(state1, state2):\n",
    "    return np.trace((np.outer(np.conj(state1.T), state1) - np.outer(np.conj(state2.T), state2)) ** 2).real\n",
    "\n",
    "\n",
    "def manhattan_dist(state1, state2):\n",
    "    return np.sum(np.abs((state1 - state2).real) + np.abs((state1 - state2).imag))\n",
    "\n",
    "\n",
    "def euclidean_dist(state1, state2):\n",
    "    return np.sum(np.abs(state1 - state2) ** 2) ** 0.5\n",
    "\n",
    "\n",
    "def hellinger_dist(state1, state2):\n",
    "    probs1 = np.abs(state1) ** 2\n",
    "    probs2 = np.abs(state2) ** 2\n",
    "    return np.sqrt(max(0, 1 - np.sum(np.sqrt((probs1 * probs2)))))\n",
    "\n",
    "\n",
    "def compute_distance_matrices(vects):\n",
    "    dm_hs = np.array([np.zeros((len(vects[0]), len(vects[0]))) for i in range(len(vects))])\n",
    "    dm_td = np.array([np.zeros((len(vects[0]), len(vects[0]))) for i in range(len(vects))])\n",
    "    dm_md = np.array([np.zeros((len(vects[0]), len(vects[0]))) for i in range(len(vects))])\n",
    "    dm_ed = np.array([np.zeros((len(vects[0]), len(vects[0]))) for i in range(len(vects))])\n",
    "    \n",
    "    for i in range(len(vects)):\n",
    "        for j in range(len(vects[0])):\n",
    "            for k in range(j, len(vects[0])):\n",
    "                dm_hs[i, j, k] = hilbert_schmidt_metric(vects[i][j], vects[i][k])\n",
    "                dm_td[i, j, k] = trace_dist(vects[i][j], vects[i][k])\n",
    "                dm_ed[i, j, k] = euclidean_dist(vects[i][j], vects[i][k])\n",
    "                dm_md[i, j, k] = manhattan_dist(vects[i][j], vects[i][k]) \n",
    "                \n",
    "    return dm_hs, dm_td, dm_md, dm_ed\n",
    "\n",
    "\n",
    "def compute_edge_distances(adj_mats):\n",
    "    return np.array([graph_shortest_path.graph_shortest_path(adj_mat, False, 'D') for adj_mat in adj_mats])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_persistent_homology(data, title):\n",
    "    \"\"\"\n",
    "    Plot persistent homology: plot persistence barcodes for every n-dim 'hole' found.\n",
    "    \"\"\"\n",
    "    max_0_dim = [max([sample[1] for sample in data[0][:-1]])]\n",
    "    data[0][-1][1] = max(max_0_dim) * 1.5\n",
    "\n",
    "    max_higher_dims = []\n",
    "    \n",
    "    for i in range(1, len(data)):\n",
    "        if len(data[i]):\n",
    "            max_higher_dims.append(max([sample[1] for sample in data[i]]))\n",
    "            \n",
    "    if len(data) - 1:\n",
    "        data[0][-1][1] = max(max_0_dim + max_higher_dims) * 1.5\n",
    "\n",
    "    plt.xlabel('Epsilon')\n",
    "    plt.title(title)\n",
    "    colors = ['darkblue', 'salmon', 'crimson', 'indigo']\n",
    "    lines = []\n",
    "    y = 0\n",
    "\n",
    "    for dim in range(len(data)):\n",
    "        lines.append(Line2D([0], [0], lw=4, color=colors[dim], label='Dimension: ' + str(dim)))\n",
    "        for element in data[dim]:\n",
    "            plt.plot([element[0], element[1]], [y, y], c=colors[dim])\n",
    "            y += 1    \n",
    "\n",
    "    plt.legend(handles=lines, loc='lower right')    \n",
    "\n",
    "\n",
    "def plot_ph_changes(dms, names, max_dim=1):\n",
    "    \"\"\"\n",
    "    Plot persistent homology changes: plot the persistance diagrams based on the transformed statevectors representing the\n",
    "    embedded data after every block.\n",
    "    \"\"\"\n",
    "    columns = len(dms)\n",
    "    rows = len(dms[0])\n",
    "    index = 1\n",
    "    plt.figure(figsize=[10 * columns, 10 * rows])\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            ph_result = ripser(dms[j][i], distance_matrix=True, maxdim=max_dim)['dgms']\n",
    "            plt.subplot(rows, columns, index)\n",
    "            plot_persistent_homology(ph_result, 'Step {} : {}'.format(i + 1, names[j]))\n",
    "            index += 1\n",
    "        \n",
    "def plot_pca_transformed_data(vects, num_samples, use_3d=True):\n",
    "    \"\"\"\n",
    "    Plot low dimensional representations of the embedded data points.\n",
    "    \"\"\"\n",
    "    dim = 3 if use_3d else 2\n",
    "    pca = PCA(n_components=dim)\n",
    "    cols = len(vects)\n",
    "    fig = plt.figure(figsize=[30, len(vects[0]) * 9])    \n",
    "        \n",
    "    for i in range(cols):\n",
    "        pca.fit(vects[i][0].real)\n",
    "        \n",
    "        for j in range(len(vects[i])):\n",
    "            real_points = pca.transform(vects[i][j].real)\n",
    "\n",
    "            if use_3d:\n",
    "                ax = fig.add_subplot(cols, len(vects[0]), j + 1, projection='3d')\n",
    "                ax.scatter(real_points[:num_samples, 0], real_points[:num_samples, 1], real_points[:num_samples, 2], c='b')\n",
    "                ax.scatter(real_points[num_samples:, 0], real_points[num_samples:, 1], real_points[num_samples:, 2], c='r')    \n",
    "            else:\n",
    "                ax = fig.add_subplot(len(vects[0]), cols, cols * j + (i + 1))\n",
    "                ax.scatter(real_points[:num_samples, 0], real_points[:num_samples, 1], c='b')\n",
    "                ax.scatter(real_points[num_samples:, 0], real_points[num_samples:, 1], c='r')    \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Area\n",
    "\n",
    "### QNN Training Area\n",
    "\n",
    "**Models Run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create parameters to feed to circuit generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_shape = {'weights': (48)}\n",
    "layers = [ry_layer, c_layer] * 12\n",
    "layer_params = [[None], ['cx', 'linear', 4]] * 12\n",
    "weight_boundaries = [0] + [4 * (i // 2) for i in range(2, 26)]\n",
    "num_qubits = 4\n",
    "circ = torch_qnn_generator(num_qubits, layers, layer_params, weight_boundaries, \n",
    "                           qml.device('default.qubit.autograd', wires=num_qubits), 1)\n",
    "\n",
    "qlayer = qml.qnn.TorchLayer(circ, weights_shape)\n",
    "\n",
    "model = TorchQNN(qlayer).to(device)\n",
    "model.load_state_dict(torch.load('../models/embedded_discs/0/quantum/structure_1/temp.txt'))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View model circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qiskit_dev = qml.device('qiskit.aer', wires=num_qubits)\n",
    "drawing_circ = torch_qnn_generator(num_qubits, layers, layer_params, weight_boundaries, qiskit_dev, 1)\n",
    "drawing_circ([0, 1], np.random.sample(96))\n",
    "qiskit_dev._circuit.draw(output='mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate, view dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "train_data, test_data = embedded_discs(num_samples, 0)\n",
    "print(train_data.shape, test_data.shape)\n",
    "plt.figure(figsize=[20, 20])\n",
    "plt.scatter(train_data[:int(0.75 * num_samples), 0], train_data[:int(0.75 * num_samples), 1], c='b')\n",
    "plt.scatter(train_data[int(0.75 * num_samples):, 0], train_data[int(0.75 * num_samples):, 1], c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert dataset to pytorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_data)\n",
    "test_dataset = CustomDataset(test_data)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set directory to store model details in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_storage_dir = '../models/embedded_discs/8/quantum/structure_1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize parameters for multiple models with the same structure, and store details of models that generalize well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desired_well_trained_models = 10\n",
    "\n",
    "for i in range(int(desired_well_trained_models * 1.5)):\n",
    "    circ = torch_qnn_generator(num_qubits, layers, layer_params, weight_boundaries, qml.device('default.qubit.autograd', \n",
    "                                                                                               wires=num_qubits), 1)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    qlayer = qml.qnn.TorchLayer(circ, weights_shape)\n",
    "    model = TorchQNN(qlayer).to(device)\n",
    "    accuracy = train_nn(model, train_data_loader, test_data_loader, opt, 100, False)\n",
    "\n",
    "    if accuracy > 0.999:\n",
    "        torch.save(model.state_dict(), model_storage_dir + 'model_' + str(i+1) + '.txt')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_cutoffs = [2 * i for i in range(13)]\n",
    "pred_vects = []\n",
    "\n",
    "for i in layer_cutoffs:\n",
    "    qml_circ = torch_qnn_generator(num_qubits, layers[:i], layer_params, weight_boundaries, qml.device('default.qubit.autograd', wires=num_qubits), 0)\n",
    "    print(i)\n",
    "    with torch.no_grad():\n",
    "        pred_vects.append(get_predictions(extra_data_loader, qml_circ, False, np.array(list(model.parameters())[0])))\n",
    "        \n",
    "pred_vects = np.array(pred_vects)\n",
    "print(pred_vects.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical NN Training Area\n",
    "\n",
    "**Models Run:**\n",
    "\n",
    "* Embedded Discs:\n",
    "    \n",
    "    * Structure 1: 5x 2-15-15-15-15-15-15-2\n",
    "    * Structure 2: 5x 2-15-15-15-15-2\n",
    "    * Structure 3: 5x 2-15-15-15-2\n",
    "    \n",
    "#### Set model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_struct = [[3, 15], [15, 15], [15, 15], [15, 15], [15, 15], [15, 15], [15, 15], [15, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create, view dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 50000\n",
    "train_data, test_data = concentric_spheres(num_samples, 9)\n",
    "\n",
    "print(train_data.shape, test_data.shape)\n",
    "fig = plt.figure(figsize=[20, 20])\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax.scatter(train_data[:int(0.75 * num_samples), 0], train_data[:int(0.75 * num_samples), 1], \n",
    "           train_data[:int(0.75 * num_samples), 2], c='b', marker='.', s=4)\n",
    "\n",
    "ax.scatter(train_data[int(0.75 * num_samples):, 0], train_data[int(0.75 * num_samples):, 1], \n",
    "           train_data[int(0.75 * num_samples):, 2], c='g', marker='.', s=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert datase into pytorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_data, False)\n",
    "test_dataset = CustomDataset(test_data, False)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set directory to store model details in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_storage_dir = '../models/concentric_spheres/9/classical/structure_1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize parameters for multiple models with the same structure, and store details of models that generalize well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_well_trained_models = 5\n",
    "\n",
    "# for i in range(int(desired_well_trained_models + 2)):\n",
    "for i in range(1):\n",
    "    c_model = ClassicalNN(model_struct)  \n",
    "\n",
    "    opt = torch.optim.Adam(c_model.parameters(), lr=0.001)\n",
    "    qlayer = qml.qnn.TorchLayer(circ, weights_shape)\n",
    "    accuracy = train_nn(c_model, train_data_loader, test_data_loader, opt, 50, False)  \n",
    "\n",
    "    if accuracy > 0.999:\n",
    "        torch.save(c_model.state_dict(), model_storage_dir + 'model_' + str(i+1) + '.txt')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = get_predictions(test_data_loader, c_model)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i, (x, y) in enumerate(test_data_loader):\n",
    "#         print('Sample: {} | Label: {} | Pred: {}'.format(x.numpy()[0], y.numpy()[0], preds[i]))\n",
    "print(list(c_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(c_model.state_dict(), '../models/embedded_discs/classical/structure_3/model_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(2, 6):\n",
    "    c_model = ClassicalNN([[15, 15], [15, 15], [15, 15]], [15, 15])  \n",
    "    opt = torch.optim.Adam(c_model.parameters(), lr=0.001)\n",
    "    train_nn(c_model, train_data_loader, test_data_loader, opt, 500, False)    \n",
    "    torch.save(c_model.state_dict(), '../models/embedded_discs/classical/structure_3/model_' + str(i) + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model.load_state_dict(torch.load('../models/embedded_discs/8/classical/structure_3/model_1.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_prediction_landscape(test_data, c_model, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "### Embedded Discs, Classical Structure 3: 2-15-15-15-15-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "pred_vects_list = []\n",
    "layers_list = [[2, 15], [15, 15], [15, 15], [15, 15], [15, 2]]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        weights = torch.load('../models/embedded_discs/8/classical/structure_3/model_' + str(i + 1) + '.txt')\n",
    "        pred_vects = []\n",
    "        \n",
    "        for i in range(0, len(layers_list)):\n",
    "            c_model = ClassicalNN(layers_list[:i + 1], False)\n",
    "            layer_dict = OrderedDict({key:weights[key] for key in list(weights.keys())[:2 * (i + 1)]})\n",
    "            c_model.load_state_dict(layer_dict, strict=True)\n",
    "            pred_vects.append(get_predictions(train_data_loader, c_model))\n",
    "            \n",
    "        pred_vects_list.append(pred_vects)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[30, 9])\n",
    "plt.scatter(train_data[:int(0.75 * num_samples), 0], train_data[:int(0.75 * num_samples), 1], c='b')\n",
    "plt.scatter(train_data[int(0.75 * num_samples):, 0], train_data[int(0.75 * num_samples):, 1], c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_transformed_data([i[:-1] for i in pred_vects_list], 15000, False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[30, 9])  \n",
    "\n",
    "for i in range(len(pred_vects_list)):\n",
    "    ax = fig.add_subplot(1, len(pred_vects_list), i + 1)  \n",
    "    ax.scatter(pred_vects_list[i][-1][:int(0.75 * num_samples), 0], pred_vects_list[i][-1][:int(0.75 * num_samples), 1], c='b')\n",
    "    ax.scatter(pred_vects_list[i][-1][int(0.75 * num_samples):, 0], pred_vects_list[i][-1][int(0.75 * num_samples):, 1], c='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded Discs, Classical Structure 1: 2-15-15-15-15-15-15-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vects_list = []\n",
    "layers_list = [[2, 15], [15, 15], [15, 15], [15, 15], [15, 15], [15, 15], [15, 2]]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        weights = torch.load('../models/embedded_discs/8/classical/structure_1/model_' + str(i + 1) + '.txt')\n",
    "        pred_vects = []\n",
    "        \n",
    "        for i in range(0, len(layers_list)):\n",
    "            c_model = ClassicalNN(layers_list[:i + 1], False)\n",
    "            layer_dict = OrderedDict({key:weights[key] for key in list(weights.keys())[:2 * (i + 1)]})\n",
    "            c_model.load_state_dict(layer_dict, strict=True)\n",
    "            pred_vects.append(get_predictions(train_data_loader, c_model))\n",
    "            \n",
    "        pred_vects_list.append(pred_vects)   \n",
    "        \n",
    "print([[j.shape for j in i] for i in pred_vects_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[30, 9])\n",
    "plt.scatter(train_data[:int(0.75 * num_samples), 0], train_data[:int(0.75 * num_samples), 1], c='b')\n",
    "plt.scatter(train_data[int(0.75 * num_samples):, 0], train_data[int(0.75 * num_samples):, 1], c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_transformed_data([i[:-1] for i in pred_vects_list], 15000, False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[30, 9])  \n",
    "\n",
    "for i in range(len(pred_vects_list)):\n",
    "    ax = fig.add_subplot(1, len(pred_vects_list), i + 1)  \n",
    "    ax.scatter(pred_vects_list[i][-1][:int(0.75 * num_samples), 0], pred_vects_list[i][-1][:int(0.75 * num_samples), 1], c='b')\n",
    "    ax.scatter(pred_vects_list[i][-1][int(0.75 * num_samples):, 0], pred_vects_list[i][-1][int(0.75 * num_samples):, 1], c='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_list = [[2, 15], [15, 15], [15, 15], [15, 15], [15, 2]]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        weights = torch.load('../models/embedded_discs/8/classical/structure_1/model_' + str(i + 1) + '.txt')\n",
    "        pred_vects = []\n",
    "        \n",
    "        for i in range(0, len(layers_list)):\n",
    "            c_model = ClassicalNN(layers_list[:i + 1], False)\n",
    "            layer_dict = OrderedDict({key:weights[key] for key in list(weights.keys())[:2 * (i + 1)]})\n",
    "            c_model.load_state_dict(layer_dict, strict=True)\n",
    "            pred_vects.append(get_predictions(train_data_loader, c_model))\n",
    "            \n",
    "        pred_vects_list.append(pred_vects)   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.16xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
